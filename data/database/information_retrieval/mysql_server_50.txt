For instance , to test the effect of differing degrees of heterogeneity , we generate dataspaces with different distributions for the number of matches in which an element participates .	0
We also experimented with different bucket sizes for the histogram technique .	0
As expected , the effectiveness is higher at smaller bucket sizes , though there was not a marked difference .	0
For all experiments with the histogram approach , we use a bucket size of 0 01 .	0
The results of this experiment are presented in Figure 6 .	0
The MySQL cluster had three machines each running one MySQL server .	1
This performance is due to the inaccurate confidences produced by SecondString and the lack of confidences for Google Base matches .	0
A slight improvement occurs when using the histogram approach to convert the confidence for the Google Base matches VPI with GB histogram .	0
The improvement is small because the conversion is very coarse-grained : all Google Base matches have no input confidence and thus get assigned to the same histogram bucket resulting in all Google Base matches getting the same output probability .	0
When the SecondString match confidences are converted to probabilities , we see a large jump in the V P I strategy s effectiveness VPI with SStr histogram .	0
Here , the histogram approach is able to map input confidences to probabilities	0

Random VPI with 0 5 for GB VPI with SStr histogram	0

Figure 6 : The performance of the V P I strategy using different means of assigning probabilities to matches , run over the Full dataset .	0
at a very fine resolution .	0
also be stored on the local disk of the EC2 machines that runs the MySQL server .	1
Results on Synthetic Datasets Using the dataspace generator , we ran a series of tests that evaluated the V P I strategy under a variety of dataspace environments .	0
REFERENCES	0
We generated a dataspace that recreates a realistic large-scale data integration environment using realistic values or distributions for different characteristics : e g , 100000 elements and a zipfian distribution for the query weights and items per element .	0
With the existing system , a single MySQL server may suddenly be handling much more load than it can handle , forcing the team to manually re-shard data from this server onto multiple servers .	1
Robustness tests .	0

We bring a MySQL server installed on a high-end laptop , which communicates to an FPGA via Gigabit Ethernet using the SIRC framework 4 see Section 2 .	1
Regardless of	0
the distribution used in this experiment , the utilities produced by the V P I strategy are within 10 of the percent of potential improvement in utility as described in the query workload experiment in the previous section .	0
We varied other parameters of the dataspace and ran similar experiments ; all experiments produced results similar to the first experiment .	0
In particular , we varied the distributions for the number items in which each element appears , introduced errors into element cardinality statistics , used different mechanism accuracies , and used different distributions for the query weights as investigated above with the Google Base datasets .	0


Finally , we restart the MySQL server and collect statistics for recovery processing .	1
Two Dell PowerEdge 1955 blade servers are used in the experiments : one is running a MySQL server with the InnoDB backend storage engine supporting a 20-warehouse TPCC database , the other is running a client driver that repeatedly issues TPCC transaction requests and measures the transaction rates .	1

before , we alter the equation for result quality to consider both precision and recall using F-measure 34 2 , defined as	0
At the demo , we will monitor CPU usage of the laptop running the MySQL server .	1
2 precision recall .	0
precision recall Precision and recall are defined in our context as follows :	0
We redefine the result quality of query Q using F-measure as follows :	0
Substituting this formula into Equation 4 , we can express the expected result quality , Er Q , D , M , when using thresholding :	0
We ran the MySQL server with the LinkBench workload under four different configurations created by turning each of the two options on and off .	1
We used MySQL Server 5 0 51 for our database system , and all algorithms were coded using C and compiled with gcc .	1
The MySQL server can be run in verbose mode , such that for every query on tables using the Ibex storage engine the parts of the query that are pushed down to the FPGA are displayed on the server console .	1
The MySQL server was run on a Linux system with an AMD Athlon XP 2600 CPU with 2GB RAM running OpenSUSE Linux 11 0 with kernel version 2 6 25 , glibc version 2 8 and GCC version 4 3 1 .	1
VPI with GB histogram VPI with full histogram	0
The algorithms were implemented in C and the database was managed using MySQL Server 5 5 27 .	1
Separate hosts were used for the LinkBench client and MySQL server .	1
The MySQL server was saturated using only a fraction of client CPU and network capacity .	1

We have tested the performance of the ColumbuScout system on a Linux machine running ubuntu12 9 and mysql Server 5 1 413 .	1
The database system used for the experiments is MySQL Server 5 0 51 ; and all algorithms are coded using C and compiled and optimized with gcc .	1
It uses MySQL server as its underlying database management system .	1
The V P I strategy is robust to variations in the dataspace characteristics : manipulating one characteristic of the dataspace e g , matches per element , items per element while leaving the others at the realistic values used in the basic tests above has very little effect on the efficacy of the V P I strategy .	0
