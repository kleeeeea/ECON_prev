{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82011\n",
      "----------------------------------------\n",
      "293632\n",
      "auto\n",
      "The hierarchical <c>pattern_graph</c> , HP G Q V , E , AN , is a <c>labeled_directed_graph</c> with V the nodes , E the <c>direct_edges</c> and AN the annotations associated with edges .\n",
      "\n",
      "econ\n",
      "The <c>graph</c> <c>Q_V</c> <c>E</c> <c>AN</c> is a labeled directed <c>graph</c> with <c>V</c> the <c>nodes</c> <c>E</c> the <c>direct_edges</c> and AN the <c>annotations</c> associated with <c>edges</c>\n",
      "\n",
      "----------------------------------------\n",
      "1240849\n",
      "----------------------------------------\n",
      "1588732\n",
      "----------------------------------------\n",
      "1312795\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/cs/student/klee/ve/lib/python2.7/site-packages/')\n",
    "\n",
    "import sys\n",
    "import random\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import cPickle\n",
    "# import _pickle as cPickle\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim import corpora, models, similarities\n",
    "import logging\n",
    "import pdb\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import average_precision_score\n",
    "import numpy as np\n",
    "\n",
    "tqdm.monitor_interval = 0\n",
    "\n",
    "dataset = 'database'\n",
    "workspaceDir = '/scratch/home/hwzha/workspace'\n",
    "query = 'xml_databases'.replace(' ', '_')\n",
    "TOPK = 50\n",
    "information_retrievalDir = '%s/evaluation/%s/information_retrieval' % (workspaceDir, dataset)\n",
    "\n",
    "APPROACHES = ['auto', 'textrank', 'kea', 'rake', 'spacy_np', 'StructMineDataPipeline', 'econ']\n",
    "\n",
    "# visualize segmentations of each approach\n",
    "approach2segmentation = {}\n",
    "for approach in APPROACHES:\n",
    "    textFile = '/scratch/home/hwzha/workspace/%s/result/%s/merged.txt_without_sentence_id.evaluation.txt' % (approach, dataset)\n",
    "    with open(textFile) as fin:\n",
    "        approach2segmentation[approach] = [i for i in fin]\n",
    "\n",
    "lineCount = len(approach2segmentation[approach])\n",
    "\n",
    "for visualize_count in np.random.randint(lineCount, size=5):\n",
    "    print(visualize_count)\n",
    "    for approach in ['auto', 'econ']:\n",
    "        if approach == 'auto':\n",
    "            if '<c>' not in approach2segmentation[approach][visualize_count]:\n",
    "                break\n",
    "        print(approach)\n",
    "        print(approach2segmentation[approach][visualize_count])\n",
    "    print('--'*20)\n",
    "\n",
    "\n",
    "def retrieve(query, approach, approach2segmentation=approach2segmentation, topN=None):\n",
    "    if approach in ['StructMineDataPipeline', 'rake', 'econ', 'kea']:\n",
    "        query_re = re.compile(r'<c>%s<\\/c>' % query)\n",
    "    else:\n",
    "        query_re = re.compile('\\b%s\\b' % query)\n",
    "    # wrapped_query_re = re.crompile(r'<c>%s<\\/c>' % query)\n",
    "    if topN:\n",
    "        return [i for i, l in enumerate(approach2segmentation[approach]) if query_re.search(l)][:topN]\n",
    "\n",
    "    return [i for i, l in enumerate(approach2segmentation[approach]) if query_re.search(l)]\n",
    "\n",
    "# information retrieval\n",
    "\n",
    "# texts\n",
    "textFile = '/scratch/home/hwzha/data/%s/merged.txt_without_sentence_id' % dataset\n",
    "with open(textFile) as fin:\n",
    "    texts = [i for i in fin]\n",
    "\n",
    "# now retrieve score list of all documents\n",
    "def retrieve_scores(query, file_pureText, dictionary, modelTfidf):\n",
    "    index = cPickle.load(open(file_pureText+'.index', 'rb'))\n",
    "    scores_matrix = index[modelTfidf[dictionary.doc2bow([query])]]\n",
    "    scores_matrix = np.array(scores_matrix)\n",
    "    return scores_matrix\n",
    "\n",
    "def remove_marker(text):\n",
    "    return re.subn(pattern='</?c>',string=text, repl='')[0]\n",
    "def wrap_marker(text):\n",
    "    text = re.subn(pattern='</?c>',string=text, repl='')[0]\n",
    "    return '<c>' + text + '</c>'\n",
    "\n",
    "def get_line_count(inFile):\n",
    "    count = -1\n",
    "    for count, line in enumerate(open(inFile, 'r')):\n",
    "        pass\n",
    "    count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:47<00:00,  6.82s/it]\n",
      "100%|██████████| 7/7 [00:46<00:00,  6.66s/it]\n"
     ]
    }
   ],
   "source": [
    "APPROACHES = ['auto', 'textrank', 'kea', 'rake', 'spacy_np', 'StructMineDataPipeline', 'econ']\n",
    "approach2indexes = {}\n",
    "\n",
    "for approach in tqdm(APPROACHES):\n",
    "    # if approach in ['StructMineDataPipeline', 'rake', 'econ', 'kea']:\n",
    "    #     query = wrap_marker(query)\n",
    "\n",
    "    file_pureText = \"%s/%s/result/%s/merged.txt_without_sentence_id.evaluation.txt\" % (workspaceDir, approach, dataset)\n",
    "    corpus = corpora.MmCorpus(file_pureText + '.corpus')\n",
    "    dictionary = corpora.Dictionary.load(file_pureText + '.dict')\n",
    "    modelTfidf = models.TfidfModel.load(file_pureText + '.modelTfidf')\n",
    "\n",
    "    # scores_matrix = retrieve(query, file_pureText=file_pureText, dictionary=dictionary, modelTfidf=modelTfidf)\n",
    "    # retreived_indexes = scores_matrix.argsort()[-TOPK:][::-1] # first pass retrieved indexes\n",
    "    retreived_indexes = retrieve(query, approach, topN=TOPK)\n",
    "    approach2indexes[approach] = retreived_indexes\n",
    "    # query = remove_marker(query)\n",
    "\n",
    "common_indexes = set.intersection(*[set(approach2indexes[approach]) for approach in APPROACHES])\n",
    "all_retreived_indexes = []\n",
    "for approach in APPROACHES:\n",
    "    retreived_indexes = [index for index in approach2indexes[approach] if index not in common_indexes]\n",
    "    all_retreived_indexes.extend(retreived_indexes)\n",
    "all_retreived_indexes = list(set(all_retreived_indexes))\n",
    "\n",
    "#second pass retrieve scores\n",
    "approach2scores = {}\n",
    "for approach in tqdm(APPROACHES):\n",
    "    if approach in ['StructMineDataPipeline', 'rake', 'econ', 'kea']:\n",
    "        query = wrap_marker(query)\n",
    "\n",
    "    file_pureText = \"%s/%s/result/%s/merged.txt_without_sentence_id.evaluation.txt\" % (workspaceDir, approach, dataset)\n",
    "    corpus = corpora.MmCorpus(file_pureText + '.corpus')\n",
    "    dictionary = corpora.Dictionary.load(file_pureText + '.dict')\n",
    "    modelTfidf = models.TfidfModel.load(file_pureText + '.modelTfidf')\n",
    "    scores_matrix = retrieve_scores(query, file_pureText, dictionary=dictionary, modelTfidf=modelTfidf)\n",
    "    all_scores = scores_matrix[all_retreived_indexes] # scores of all_retreived_indexes\n",
    "    approach2scores[approach] = all_scores\n",
    "    query = remove_marker(query)\n",
    "\n",
    "with open('%s/%s_%s.txt' % (information_retrievalDir, query, TOPK), 'w') as f_out, \\\n",
    "    open('%s/%s_%s_explanation.txt' % (information_retrievalDir, query, TOPK), 'w') as f_out_explanation:\n",
    "        for i, index in enumerate(all_retreived_indexes):\n",
    "            score_explanation = []\n",
    "            for approach in APPROACHES:\n",
    "                score_explanation.append(approach + ':' + str(approach2scores[approach][i]))\n",
    "            score_explanation = ','.join(score_explanation)\n",
    "            f_out.write(texts[index])\n",
    "            f_out_explanation.write(texts[index].strip() + '\\t' + str(index) + '\\t' + score_explanation + '\\n')\n",
    "\n",
    "with open('%s/%s_%s_order.txt' % (information_retrievalDir, query, TOPK), 'w') as f_out_order:\n",
    "    indexs = [str(i) for i in all_retreived_indexes]\n",
    "    f_out_order.write(','.join(indexs) + '\\n')\n",
    "    for approach in APPROACHES:\n",
    "        scores = approach2scores[approach].tolist()\n",
    "        scores = [str(s) for s in scores]\n",
    "        f_out_order.write(approach + '\\t'  + ','.join(scores) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_autophrase_output = \"/scratch/home/hwzha/workspace/AutoPhrase/models/%s/segmentation.txt\" % (dataset)\n",
    "auto_text = []\n",
    "with open(raw_autophrase_output) as f:\n",
    "    for line in f:\n",
    "        auto_text.append(line.replace('<phrase>', '<c>').replace('</phrase>', '</c>'))\n",
    "approach2segmentation['auto'] = auto_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1608407\n",
      "auto\n",
      "We first introduce a new concept , called subgraph <c>core number</c> , which is geared to the main idea of Inc-S .\n",
      "\n",
      "econ\n",
      "We first introduce a <c>new_concept</c> called <c>core_number</c> which is geared to the <c>main_idea</c> of <c>Inc-S</c>\n",
      "\n",
      "StructMineDataPipeline\n",
      "We first introduce a <c>new_concept</c> , called <c>subgraph_core_number</c> , which is geared to the <c>main_idea</c> of <c>Inc-S</c> .\n",
      "\n",
      "----------------------------------------\n",
      "1434997\n",
      "auto\n",
      "5 3 <c>Solver Derivation</c> Rules\n",
      "\n",
      "econ\n",
      "<c>Solver_Derivation_Rules</c>\n",
      "\n",
      "StructMineDataPipeline\n",
      "5 3 <c>Solver_Derivation_Rules</c>\n",
      "\n",
      "----------------------------------------\n",
      "165085\n",
      "----------------------------------------\n",
      "1144595\n",
      "----------------------------------------\n",
      "1265520\n",
      "auto\n",
      "<c>Snapshot Isolation SI</c> has become a popular <c>isolation level</c> in database systems .\n",
      "\n",
      "econ\n",
      "<c>SI</c> has become a <c>isolation_level</c> in <c>database_systems</c>\n",
      "\n",
      "StructMineDataPipeline\n",
      "<c>Snapshot_Isolation_SI</c> has become a <c>popular_isolation_level</c> in <c>database_systems</c> .\n",
      "\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def visualize_by_index(index):\n",
    "    for approach in ['auto', 'econ', 'StructMineDataPipeline']:\n",
    "        if approach == 'auto':\n",
    "            if '<c>' not in approach2segmentation[approach][index]:\n",
    "                break\n",
    "        print(approach)\n",
    "        print(approach2segmentation[approach][index])\n",
    "    print('--'*20)\n",
    "\n",
    "for visualize_count in np.random.randint(lineCount, size=5):\n",
    "    print(visualize_count)\n",
    "    visualize_by_index(visualize_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(363059,\n",
       "  'Graphs-at-a-time : <c>Query Language</c> and Access Methods for <c>Graph Databases</c>\\n')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i,l) for i,l in enumerate(approach2segmentation['auto']) if l.startswith('Graphs-at-a-time')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto\n",
      "Graphs-at-a-time : <c>Query Language</c> and Access Methods for <c>Graph Databases</c>\n",
      "\n",
      "econ\n",
      "<c>Graphs-at-a-time</c> <c>Query_Language</c> and <c>Access_Methods</c> for <c>Graph_Databases</c>\n",
      "\n",
      "StructMineDataPipeline\n",
      "<c>Graphs-at-a-time</c> : <c>Query_Language</c> and <c>Access_Methods</c> for <c>Graph_Databases</c>\n",
      "\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "visualize_by_index(363059)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py2",
   "language": "python",
   "name": "py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
