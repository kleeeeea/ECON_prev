{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"gensim.models.doc2vec\"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import random\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import cPickle\n",
    "# import _pickle as cPickle\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim import corpora, models, similarities\n",
    "import logging\n",
    "import pdb\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import average_precision_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tqdm.monitor_interval = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = 'machine_learning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "workspaceDir = '/scratch/home/hwzha/workspace'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "APPROACHES = ['auto', 'textrank', 'kea', 'rake', 'spacy_np', 'StructMineDataPipeline', 'econ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize segmentations of each approach\n",
    "approach2segmentation = {}\n",
    "for approach in APPROACHES:\n",
    "    textFile = '/scratch/home/hwzha/workspace/%s/result/%s/merged.txt_without_sentence_id.evaluation.txt' % (approach, dataset)\n",
    "    with open(textFile) as fin:\n",
    "        approach2segmentation[approach] = [i for i in fin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lineCount = len(approach2segmentation[approach])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244602\n",
      "auto\n",
      "27 <c>Chris_J_Maddison</c> , <c>Andriy_Mnih</c> , and Yee Whye Teh .\n",
      "\n",
      "econ\n",
      "<c>Chris_J_Maddison</c> <c>Andriy_Mnih</c> and <c>Whye_Teh</c>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for visualize_count in np.random.randint(lineCount, size=5):\n",
    "visualize_count = 244602\n",
    "print(visualize_count)\n",
    "for approach in ['auto', 'econ']:\n",
    "    if approach == 'auto':\n",
    "        if '<c>' not in approach2segmentation[approach][visualize_count]:\n",
    "            break\n",
    "    print(approach)\n",
    "    print(approach2segmentation[approach][visualize_count])\n",
    "# print('--'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, approach, approach2segmentation=approach2segmentation, topN=None):\n",
    "#     if approach in ['StructMineDataPipeline', 'rake', 'econ', 'kea']:\n",
    "    query_re = re.compile(r'<c>%s<\\/c>' % query)\n",
    "#     else:\n",
    "#         query_re = re.compile('\\b%s\\b' % query)\n",
    "    # wrapped_query_re = re.crompile(r'<c>%s<\\/c>' % query)\n",
    "    if topN:\n",
    "        return [i for i,l in enumerate(approach2segmentation[approach]) if query_re.search(l)][:topN]\n",
    "\n",
    "    return [i for i,l in enumerate(approach2segmentation[approach]) if query_re.search(l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['econ',\n",
       " 'auto',\n",
       " 'rake',\n",
       " 'kea',\n",
       " 'StructMineDataPipeline',\n",
       " 'spacy_np',\n",
       " 'textrank']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approach2segmentation.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# information retrieval \n",
    "\n",
    "# texts\n",
    "textFile = '/scratch/home/hwzha/data/%s/merged.txt_without_sentence_id' % dataset\n",
    "with open(textFile) as fin:\n",
    "    texts = [i for i in fin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now retrieve score list of all documents \n",
    "def retrieve(query, file_pureText, dictionary, modelTfidf):\n",
    "    index = cPickle.load(open(file_pureText+'.index', 'rb'))\n",
    "    scores_matrix = index[modelTfidf[dictionary.doc2bow([query])]]\n",
    "    scores_matrix = np.array(scores_matrix)\n",
    "    return scores_matrix\n",
    "\n",
    "def remove_marker(text):\n",
    "    return re.subn(pattern='</?c>',string=text, repl='')[0]\n",
    "def wrap_marker(text):\n",
    "    text = re.subn(pattern='</?c>',string=text, repl='')[0]\n",
    "    return '<c>' + text + '</c>'\n",
    "\n",
    "def get_line_count(inFile):\n",
    "    count = -1\n",
    "    for count, line in enumerate(open(inFile, 'r')):\n",
    "        pass\n",
    "    count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = 'flash_device'.replace(' ', '_')\n",
    "TOPK = 50\n",
    "\n",
    "APPROACHES = ['auto', 'textrank', 'kea', 'rake', 'spacy_np', 'StructMineDataPipeline', 'econ']\n",
    "approach2indexes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:39<00:00,  5.59s/it]\n",
      "100%|██████████| 7/7 [00:37<00:00,  5.31s/it]\n"
     ]
    }
   ],
   "source": [
    "for approach in tqdm(APPROACHES):\n",
    "    if approach in ['StructMineDataPipeline', 'rake', 'econ', 'kea']:\n",
    "        query = wrap_marker(query)\n",
    "    \n",
    "    file_pureText = \"%s/%s/result/%s/merged.txt_without_sentence_id.evaluation.txt\" % (workspaceDir, approach, dataset)\n",
    "    corpus = corpora.MmCorpus(file_pureText + '.corpus')\n",
    "    dictionary = corpora.Dictionary.load(file_pureText + '.dict')\n",
    "    modelTfidf = models.TfidfModel.load(file_pureText + '.modelTfidf')\n",
    "    \n",
    "    scores_matrix = retrieve(query, file_pureText=file_pureText, dictionary=dictionary, modelTfidf=modelTfidf)\n",
    "    retreived_indexes = scores_matrix.argsort()[-TOPK:][::-1] # first pass retrieved indexes\n",
    "    approach2indexes[approach] = retreived_indexes\n",
    "    query = remove_marker(query) \n",
    "\n",
    "common_indexes = set.intersection(*[set(approach2indexes[approach]) for approach in APPROACHES])\n",
    "all_retreived_indexes = []\n",
    "for approach in APPROACHES:\n",
    "    retreived_indexes = [index for index in approach2indexes[approach] if index not in common_indexes]\n",
    "    all_retreived_indexes.extend(retreived_indexes)\n",
    "all_retreived_indexes = list(set(all_retreived_indexes))\n",
    "\n",
    "new_all_retreived_indexes = []\n",
    "all_retreived_texts = []\n",
    "\n",
    "for index in all_retreived_indexes:\n",
    "    if texts[index] not in all_retreived_texts:\n",
    "        all_retreived_texts.append(texts[index])\n",
    "        new_all_retreived_indexes.append(index)\n",
    "all_retreived_indexes = new_all_retreived_indexes\n",
    "\n",
    "#second pass retrieve scores\n",
    "approach2scores = {}\n",
    "for approach in tqdm(APPROACHES):\n",
    "    if approach in ['StructMineDataPipeline', 'rake', 'econ', 'kea']:\n",
    "        query = wrap_marker(query)\n",
    "    \n",
    "    file_pureText = \"%s/%s/result/%s/merged.txt_without_sentence_id.evaluation.txt\" % (workspaceDir, approach, dataset)\n",
    "    corpus = corpora.MmCorpus(file_pureText + '.corpus')\n",
    "    dictionary = corpora.Dictionary.load(file_pureText + '.dict')\n",
    "    modelTfidf = models.TfidfModel.load(file_pureText + '.modelTfidf')\n",
    "    scores_matrix = retrieve(query, file_pureText=file_pureText, dictionary=dictionary, modelTfidf=modelTfidf)\n",
    "    all_scores = scores_matrix[all_retreived_indexes] # scores of all_retreived_indexes\n",
    "    approach2scores[approach] = all_scores\n",
    "    query = remove_marker(query) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "if len(all_retreived_indexes) > 100: \n",
    "    all_retreived_indexes = np.random.choice(all_retreived_indexes, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_retrievalDir =  '%s/evaluation/%s/information_retrieval' % (workspaceDir, dataset)\n",
    "with open('%s/%s_%s.txt' % (information_retrievalDir, query, TOPK), 'w') as f_out, \\\n",
    "    open('%s/%s_%s_explanation.txt'% (information_retrievalDir, query, TOPK), 'w') as f_out_explanation:\n",
    "        for i, index in enumerate(all_retreived_indexes):\n",
    "            score_explanation = []\n",
    "            for approach in APPROACHES:\n",
    "                score_explanation.append(approach + ':' + str(approach2scores[approach][i]))\n",
    "            score_explanation = ','.join(score_explanation)\n",
    "            f_out.write(texts[index].strip() + '\\n')\n",
    "            f_out_explanation.write(texts[index].strip() + '\\t' + str(index) + '\\t' + score_explanation + '\\n')\n",
    "\n",
    "with open('%s/%s_%s_order.txt'% (information_retrievalDir, query, TOPK), 'w') as f_out_order:\n",
    "    indexs = [str(i) for i in all_retreived_indexes]\n",
    "    f_out_order.write(','.join(indexs) + '\\n')\n",
    "    for approach in APPROACHES:\n",
    "        scores = approach2scores[approach].tolist()\n",
    "        scores = [str(s) for s in scores]\n",
    "        f_out_order.write(approach + '\\t'  + ','.join(scores) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # test label\n",
    "# lineCount = get_line_count('%s/%s_%s.txt' % (information_retrievalDir, query, TOPK))\n",
    "# with open('%s/%s_%s_label.txt'% (information_retrievalDir, query, TOPK), 'w') as f_out_label:\n",
    "#     for l in range(lineCount):\n",
    "#         label = int(round(np.random.rand()))\n",
    "#         f_out_label.write(str(label))\n",
    "#         f_out_label.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels = []\n",
    "# with open('%s/%s_%s_label.txt'% (information_retrievalDir, query, TOPK)) as f_in_label:\n",
    "#     for line in f_in_label:\n",
    "#         try:\n",
    "#             label = int(line.strip())\n",
    "#             labels.append(label)\n",
    "#         except:\n",
    "#             pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_approach2scores_and_labels(eval_query, dataset):\n",
    "    labels = []\n",
    "    information_retrievalDir =  '%s/evaluation/%s/information_retrieval' % (workspaceDir, dataset)\n",
    "    # sentence to label\n",
    "    sentence2label = {}\n",
    "    with open('%s/%s_%s.txt'% (information_retrievalDir, eval_query, TOPK)) as f_in_label:\n",
    "        for i, line in enumerate(f_in_label):\n",
    "            try:\n",
    "                if '\\t' not in line:\n",
    "                    continue\n",
    "                if len(line.strip().split('\\t')) == 1:\n",
    "                    continue\n",
    "\n",
    "                sentence, label = line.strip().split('\\t')\n",
    "                label = int(label)\n",
    "                labels.append(label)\n",
    "                if sentence in sentence2label:\n",
    "                    if sentence2label[sentence] != label:\n",
    "                        print(sentence)\n",
    "                        continue\n",
    "                else:\n",
    "                    sentence2label[sentence] = label\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(line)\n",
    "                pdb.set_trace()\n",
    "                pass\n",
    "\n",
    "    # index to label <- from explanation.txt and sentence2label\n",
    "    index2label = {}\n",
    "    indexes = []\n",
    "    with open('%s/%s_%s_explanation.txt'% (information_retrievalDir, eval_query, TOPK)) as f_in_explanation:\n",
    "        for line in f_in_explanation:\n",
    "            sentence, index, _ = line.split('\\t')\n",
    "            sentence = sentence.strip()\n",
    "            index = int(index)\n",
    "            if sentence in sentence2label:\n",
    "                indexes.append(index)\n",
    "                index2label[index] = int(sentence2label[sentence])\n",
    "\n",
    "    # approach's index to score\n",
    "    approach2index2socre = {}\n",
    "    with open('%s/%s_%s_order.txt'% (information_retrievalDir, eval_query, TOPK)) as f_in_order:\n",
    "        line = f_in_order.readline()\n",
    "        indexes = line.strip().split(',')\n",
    "        indexes = [int(i) for i in indexes]\n",
    "        for line in f_in_order:\n",
    "            approach, scores = line.split('\\t')\n",
    "            scores = scores.split(',')\n",
    "            scores = [float(s) for s in scores]\n",
    "            for index, score in zip(indexes, scores):\n",
    "    #             approach2index2socre.setdefault(approach, {}).update({index, score})\n",
    "                approach2index2socre.setdefault(approach, {})\n",
    "    #             approach2index2socre[approach][index] = score\n",
    "    #             marked_query = '<c>{}</c>'.format(eval_query)\n",
    "                marked_query = '(<c>%s</c>)|( %s )'% (eval_query, eval_query)\n",
    "                if approach == 'econ':\n",
    "                    approach2index2socre[approach][index] = score\n",
    "                elif re.search(marked_query, ' ' + approach2segmentation[approach][index] + ' ', flags=re.I):\n",
    "                    approach2index2socre[approach][index] = 1.0\n",
    "                else:\n",
    "                    approach2index2socre[approach][index] = 0.0\n",
    "\n",
    "            # indexes in ground truth sentences\n",
    "\n",
    "    approach2scores = {} \n",
    "    for approach in APPROACHES:\n",
    "        approach2scores.setdefault(approach, [])\n",
    "        for index in index2label:\n",
    "            approach2scores[approach].append(approach2index2socre[approach].get(index, 0))\n",
    "        if set(approach2scores[approach]) == set([0]):\n",
    "            print('{} not found by {} .'.format(eval_query, approach))\n",
    "    labels = index2label.values()\n",
    "    return approach2scores, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for index in index2label:\n",
    "#     print(index)\n",
    "#     print(approach2segmentation['kea'][index])\n",
    "#     print(approach2index2socre['kea'][index])\n",
    "#     print(index2label[index])\n",
    "#     print('--'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TOPK = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_query_list = ['belief_propagation', 'global_convergence', 'local_search', 'symmetric_positive_definite_matrix', 'poisson_regression']\n",
    "\n",
    "# eval_query_list = ['dna_repair', 'blood_pressures', 'drug_discovery', 'bone_marrow_transplantation', 'rna_molecules']\n",
    "\n",
    "# eval_query_list =  ['random_process', 'hash_join', 'experimental_evaluation', 'xml_databases', 'relational_database']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust max-product belief propagation .\n",
      "belief_propagation not found by rake .\n",
      "('econ', 0.9362237089509816)\n",
      "('auto', 0.671791914920879)\n",
      "('rake', 0.5454545454545454)\n",
      "('kea', 0.727483164983165)\n",
      "('StructMineDataPipeline', 0.6656071201525747)\n",
      "('spacy_np', 0.8543101469237833)\n",
      "('textrank', 0.6034369670733307)\n",
      "----------------------------------------\n",
      "global_convergence not found by rake .\n",
      "('econ', 0.9788306611164748)\n",
      "('auto', 0.8660374690703428)\n",
      "('rake', 0.8414634146341463)\n",
      "('kea', 0.8937534716962077)\n",
      "('StructMineDataPipeline', 0.9444254722576703)\n",
      "('spacy_np', 0.9241781548250265)\n",
      "('textrank', 0.8357047934816166)\n",
      "----------------------------------------\n",
      "local_search not found by rake .\n",
      "('econ', 0.9805020811654527)\n",
      "('auto', 0.8540322580645161)\n",
      "('rake', 0.775)\n",
      "('kea', 0.8110259122157588)\n",
      "('StructMineDataPipeline', 0.9333936800526662)\n",
      "('spacy_np', 0.9080453149001535)\n",
      "('textrank', 0.7721978358513679)\n",
      "----------------------------------------\n",
      "symmetric_positive_definite_matrix not found by textrank .\n",
      "symmetric_positive_definite_matrix not found by kea .\n",
      "symmetric_positive_definite_matrix not found by rake .\n",
      "('econ', 0.9897058823529412)\n",
      "('auto', 0.5936651583710408)\n",
      "('rake', 0.5882352941176471)\n",
      "('kea', 0.5882352941176471)\n",
      "('StructMineDataPipeline', 0.9176470588235295)\n",
      "('spacy_np', 0.5985294117647059)\n",
      "('textrank', 0.5882352941176471)\n",
      "----------------------------------------\n",
      "poisson_regression not found by rake .\n",
      "('econ', 0.9736218015394217)\n",
      "('auto', 0.8571114240450626)\n",
      "('rake', 0.8260869565217391)\n",
      "('kea', 0.8903706041715984)\n",
      "('StructMineDataPipeline', 0.9431816255614882)\n",
      "('spacy_np', 0.9298245614035088)\n",
      "('textrank', 0.8480919030118115)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "approach2ap = {}\n",
    "# dataset = 'machine_learning'\n",
    "for eval_query in eval_query_list:\n",
    "    approach2scores, y_true = get_approach2scores_and_labels(eval_query, dataset)\n",
    "    for approach, y_scores in approach2scores.items():\n",
    "        ap = average_precision_score(y_true, y_scores)\n",
    "        approach2ap.setdefault(approach, []).append(ap)\n",
    "        print(approach, ap)\n",
    "    print('--'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine_learning\n",
      "('auto', 0.7685276448943682)\n",
      "('textrank', 0.7295333587071549)\n",
      "('kea', 0.7821736894368755)\n",
      "('rake', 0.7152480421456155)\n",
      "('spacy_np', 0.8429775179634357)\n",
      "('StructMineDataPipeline', 0.8808509913695858)\n",
      "('econ', 0.9717768270250543)\n"
     ]
    }
   ],
   "source": [
    "print('{}'.format(dataset))\n",
    "for approach in APPROACHES:\n",
    "    print(approach, np.mean(approach2ap[approach]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# approach2scores = {}\n",
    "# approach2index2socre = {}\n",
    "# with open('%s/%s_%s_order.txt'% (information_retrievalDir, eval_query, TOPK)) as f_in_order:\n",
    "#     line = f_in_order.readline()\n",
    "#     indexes = line.strip().split(',')\n",
    "#     indexes = [int(i) for i in indexes]\n",
    "#     # indexes in labels\n",
    "#     for line in f_in_order:\n",
    "#         approach, scores = line.split('\\t')\n",
    "#         scores = scores.split(',')\n",
    "#         scores = [float(s) for s in scores]\n",
    "#         for \n",
    "#         approach2scores[approach] = scores\n",
    "#         approach2index2socre[approach].setdefault(index, {}) = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.metrics import average_precision_score\n",
    "# y_true = np.array([0, 0, 1, 1])\n",
    "# y_scores = np.array([1, 4, 3.5, 0])\n",
    "# average_precision_score(y_true, y_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py2",
   "language": "python",
   "name": "py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
