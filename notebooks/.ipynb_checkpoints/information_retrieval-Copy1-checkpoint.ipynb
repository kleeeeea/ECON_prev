{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import cPickle\n",
    "# import _pickle as cPickle\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim import corpora, models, similarities\n",
    "import logging\n",
    "import pdb\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import average_precision_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = 'machine_learning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "workspaceDir = '/scratch/home/hwzha/workspace'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q_learning'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# information retrieval \n",
    "\n",
    "# texts\n",
    "textFile = '/scratch/home/hwzha/data/%s/merged.txt_without_sentence_id' % dataset\n",
    "with open(textFile) as fin:\n",
    "    texts = [i for i in fin]\n",
    "\n",
    "# APPROACHES = ['auto', 'textrank', 'kea', 'rake', 'spacy_np', 'StructMineDataPipeline']\n",
    "\n",
    "# def retrieve(query, dictionary, modelTfidf, TOPK=10):\n",
    "def retrieve(query, dictionary, modelTfidf):\n",
    "    index = cPickle.load(open(file_pureText+'.index', 'rb'))\n",
    "#     query = 'deep_learning'\n",
    "\n",
    "    scores_matrix = index[modelTfidf[dictionary.doc2bow([query])]]\n",
    "    scores_matrix = np.array(scores_matrix)\n",
    "#     scores_indexes = scores_matrix.argsort()[-TOPK:][::-1]\n",
    "#     return scores_indexes\n",
    "    return scores_matrix\n",
    "\n",
    "def remove_marker(text):\n",
    "    return re.subn(pattern='</?c>',string=text, repl='')[0]\n",
    "def wrap_marker(text):\n",
    "    text = re.subn(pattern='</?c>',string=text, repl='')[0]\n",
    "    return '<c>' + text + '</c>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:36<00:00,  6.12s/it]\n"
     ]
    }
   ],
   "source": [
    "approach2indexes = {}\n",
    "query = 'q learning'.replace(' ', '_')\n",
    "TOPK = 50\n",
    "\n",
    "APPROACHES = ['auto', 'textrank', 'kea', 'rake', 'spacy_np', 'StructMineDataPipeline']\n",
    "\n",
    "for approach in tqdm(APPROACHES):\n",
    "    if approach in ['StructMineDataPipeline', 'rake']:\n",
    "        query = wrap_marker(query)\n",
    "    \n",
    "    file_pureText = \"%s/%s/result/%s/merged.txt_without_sentence_id.evaluation.txt\" % (workspaceDir, approach, dataset)\n",
    "    corpus = corpora.MmCorpus(file_pureText + '.corpus')\n",
    "    dictionary = corpora.Dictionary.load(file_pureText + '.dict')\n",
    "    modelTfidf = models.TfidfModel.load(file_pureText + '.modelTfidf')\n",
    "    \n",
    "    scores_matrix = retrieve(query, dictionary=dictionary, modelTfidf=modelTfidf)\n",
    "    retreived_indexes = scores_matrix.argsort()[-TOPK:][::-1]\n",
    "#     retreived_indexes = retrieve(query, dictionary=dictionary, modelTfidf=modelTfidf, TOPK=TOPK)\n",
    "    approach2indexes[approach] = retreived_indexes\n",
    "    \n",
    "    query = remove_marker(query) \n",
    "    \n",
    "indexes_explain = {}\n",
    "common_indexes = set.intersection(*[set(approach2indexes[approach]) for approach in APPROACHES])\n",
    "\n",
    "all_retreived_indexes = []\n",
    "for approach in APPROACHES:\n",
    "    retreived_indexes = [index for index in approach2indexes[approach] if index not in common_indexes]\n",
    "    all_retreived_indexes.extend(retreived_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_retreived_indexes = list(set(all_retreived_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "approach2indexes = {}\n",
    "approach2scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for approach in tqdm(APPROACHES):\n",
    "    if approach in ['StructMineDataPipeline', 'rake']:\n",
    "        query = wrap_marker(query)\n",
    "    \n",
    "    file_pureText = \"%s/%s/result/%s/merged.txt_without_sentence_id.evaluation.txt\" % (workspaceDir, approach, dataset)\n",
    "    corpus = corpora.MmCorpus(file_pureText + '.corpus')\n",
    "    dictionary = corpora.Dictionary.load(file_pureText + '.dict')\n",
    "    modelTfidf = models.TfidfModel.load(file_pureText + '.modelTfidf')\n",
    "    scores_matrix = retrieve(query, dictionary=dictionary, modelTfidf=modelTfidf)\n",
    "    all_scores = scores_matrix[all_retreived_indexes]\n",
    "    approach2scores[approach] = all_scores\n",
    "    approach2indexes[approach] = all_retreived_indexes\n",
    "    query = remove_marker(query) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index in retreived_indexes:\n",
    "#     approach2indexes_order.setdefault(approach, []).append(index)\n",
    "#     indexes_explain.setdefault(index, []).append(approach)\n",
    "\n",
    "information_retrievalDir =  '%s/evaluation/%s/information_retrieval' % (workspaceDir, dataset)\n",
    "with open('%s/%s_%s.txt' % (information_retrievalDir, query, TOPK), 'w') as f_out, \\\n",
    "    open('%s/%s_%s_explanation.txt'% (information_retrievalDir, query, TOPK), 'w') as f_out_explanation:\n",
    "        for index, source in indexes_explain.items():\n",
    "#         print(texts[index].strip() + '\\t' + ','.join(source) + '\\n')\n",
    "            f_out.write(texts[index])\n",
    "            f_out_explanation.write(texts[index].strip() + '\\t' + str(index) + '\\t' + ','.join(source) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('%s/%s_%s_order.txt'% (information_retrievalDir, query, TOPK), 'w') as f_out_order:\n",
    "    for approach in APPROACHES:\n",
    "#         indexes = approach2indexes[approach].tolist()\n",
    "        indexes = approach2indexes[approach]\n",
    "#         indexes = [str(i) for i in indexes if i not in common_indexes]\n",
    "        indexes = [str(i) for i in indexes]\n",
    "        scores = approach2scores[approach].tolist()\n",
    "        scores = [str(s) for s in scores]\n",
    "        f_out_order.write(approach + '\\t' + ','.join(indexes) + '\\t' + ','.join(scores) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_line_count(inFile):\n",
    "    count = -1\n",
    "    for count, line in enumerate(open(inFile, 'r')):\n",
    "        pass\n",
    "    count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test label\n",
    "lineCount = get_line_count('%s/%s_%s.txt' % (information_retrievalDir, query, TOPK))\n",
    "with open('%s/%s_%s_label.txt'% (information_retrievalDir, query, TOPK), 'w') as f_out_label:\n",
    "    for l in range(lineCount):\n",
    "        label = int(round(np.random.rand()))\n",
    "        f_out_label.write(str(label))\n",
    "        f_out_label.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index2labels = {}\n",
    "with open('%s/%s_%s_explanation.txt'% (information_retrievalDir, query, TOPK)) as f_in_explanation, \\\n",
    "    open('%s/%s_%s_label.txt'% (information_retrievalDir, query, TOPK)) as f_in_label:\n",
    "    for line, label in zip(f_in_explanation, f_in_label):\n",
    "        try:\n",
    "            _, index, source = line.rsplit('\\t', 2)\n",
    "            label = bool(int(label.strip()))\n",
    "            index = int(index)\n",
    "            source = source.strip()\n",
    "            index2labels[index] = label\n",
    "            \n",
    "        except:\n",
    "            pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "approach2labels = {}\n",
    "inds = []\n",
    "with open('%s/%s_%s_order.txt'% (information_retrievalDir, query, TOPK)) as f_in_order:\n",
    "    for line in f_in_order:\n",
    "        approach, indexes = line.split('\\t')\n",
    "        indexes = indexes.split(',')\n",
    "        indexes = [int(i) for i in indexes]\n",
    "        approach2labels[approach] = [index2labels[index] for index in indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.metrics import average_precision_score\n",
    "# y_true = np.array([0, 0, 1, 1])\n",
    "# y_scores = np.array([1, 4, 3.5, 0])\n",
    "# average_precision_score(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7664167266982796\n",
      "0.6428667603156227\n",
      "0.7664167266982796\n",
      "0.6740910496051588\n",
      "0.6968104922267988\n",
      "0.6700312255802272\n"
     ]
    }
   ],
   "source": [
    "for approach, y_true in approach2labels.items():\n",
    "    y_scores = np.arange(0, len(y_true))\n",
    "    ap = average_precision_score(y_true, y_scores)\n",
    "    print(ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py2",
   "language": "python",
   "name": "py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
